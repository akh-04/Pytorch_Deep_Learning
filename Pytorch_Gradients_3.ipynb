{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMr0XygajurcZ3LdPYYd5bp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akh-04/Pytorch_Deep_Learning/blob/main/Pytorch_Gradients_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xnVgt0kN54qN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing Gradient using Numpy"
      ],
      "metadata": {
        "id": "DNd2K-8V9EIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# f = w * x\n",
        "# f = 2 * x\n",
        "\n",
        "X = np.array([1,2,3,4], dtype = np.float32)\n",
        "Y = np.array([2,4,6,8], dtype = np.float32)\n",
        "\n",
        "w = 0\n",
        "# model prediction\n",
        "def frwd(x):\n",
        "  return w*x\n",
        "\n",
        "# Loss = MSE\n",
        "def loss(y, y_predicted):\n",
        "  return((y_predicted - y)**2).mean()\n",
        "\n",
        "# gradient\n",
        "# MSE = 1/M *(w*x-y)**2\n",
        "# dJ/dw = 1/N 2x (w*x-y)\n",
        "\n",
        "def gradient(x,y,y_predicted):\n",
        "  return np.dot(2*x, y_predicted-y).mean()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_TWfTabO5-Cr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Prediction before training: f(5) = {frwd(5):.3f}')\n",
        "\n",
        "# setup\n",
        "learning_rate = 0.01\n",
        "n_iters= 10\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  # prediction = forward pass\n",
        "  y_pred = frwd(X)\n",
        "  # loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  # gradients\n",
        "\n",
        "  dw = gradient(X,Y,y_pred)\n",
        "\n",
        "  w -= learning_rate * dw  \n",
        "\n",
        "  if epoch%1 == 0:\n",
        "    print(f'epoch{epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {frwd(5):.3f}') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRTQTYT37Pv0",
        "outputId": "77693a06-d501-4d34-a7b0-5b016cad2c11"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 0.000\n",
            "epoch1: w = 1.200, loss = 30.00000000\n",
            "epoch2: w = 1.680, loss = 4.79999924\n",
            "epoch3: w = 1.872, loss = 0.76800019\n",
            "epoch4: w = 1.949, loss = 0.12288000\n",
            "epoch5: w = 1.980, loss = 0.01966083\n",
            "epoch6: w = 1.992, loss = 0.00314574\n",
            "epoch7: w = 1.997, loss = 0.00050331\n",
            "epoch8: w = 1.999, loss = 0.00008053\n",
            "epoch9: w = 1.999, loss = 0.00001288\n",
            "epoch10: w = 2.000, loss = 0.00000206\n",
            "Prediction after training: f(5) = 9.999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing Gradient Desc using Pytorch"
      ],
      "metadata": {
        "id": "MwXdmXOh9Jum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X = torch.tensor([1,2,3,4], dtype = torch.float32)\n",
        "Y = torch.tensor([2,4,6,8], dtype = torch.float32)\n",
        "\n",
        "w = torch.tensor(0.0, dtype = torch.float32, requires_grad=True)\n",
        "# model prediction\n",
        "def frwd(x):        \n",
        "  return w*x\n",
        "\n",
        "# Loss = MSE\n",
        "def loss(y, y_predicted):\n",
        "  return((y_predicted - y)**2).mean()\n"
      ],
      "metadata": {
        "id": "RqsTcDxY8pGi"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Prediction before training: f(5) = {frwd(5):.3f}')\n",
        "\n",
        "# setup\n",
        "learning_rate = 0.01\n",
        "n_iters= 100\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  # prediction = forward pass\n",
        "  y_pred = frwd(X)\n",
        "  # loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  # gradients\n",
        "  l.backward() #dl/dw\n",
        "  with torch.no_grad():\n",
        "    w -= learning_rate * w.grad \n",
        "  # Empty the gradient to avoid accumalation\n",
        "  w.grad.zero_()\n",
        "\n",
        "  if epoch%1 == 0:\n",
        "    print(f'epoch{epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {frwd(5):.3f}') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxmzlz189q7s",
        "outputId": "16bc7117-649b-4858-ef1b-73c8e1c0e092"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 0.000\n",
            "epoch1: w = 0.300, loss = 30.00000000\n",
            "epoch2: w = 0.555, loss = 21.67499924\n",
            "epoch3: w = 0.772, loss = 15.66018772\n",
            "epoch4: w = 0.956, loss = 11.31448650\n",
            "epoch5: w = 1.113, loss = 8.17471695\n",
            "epoch6: w = 1.246, loss = 5.90623236\n",
            "epoch7: w = 1.359, loss = 4.26725292\n",
            "epoch8: w = 1.455, loss = 3.08308983\n",
            "epoch9: w = 1.537, loss = 2.22753215\n",
            "epoch10: w = 1.606, loss = 1.60939169\n",
            "epoch11: w = 1.665, loss = 1.16278565\n",
            "epoch12: w = 1.716, loss = 0.84011245\n",
            "epoch13: w = 1.758, loss = 0.60698116\n",
            "epoch14: w = 1.794, loss = 0.43854395\n",
            "epoch15: w = 1.825, loss = 0.31684780\n",
            "epoch16: w = 1.851, loss = 0.22892261\n",
            "epoch17: w = 1.874, loss = 0.16539653\n",
            "epoch18: w = 1.893, loss = 0.11949898\n",
            "epoch19: w = 1.909, loss = 0.08633806\n",
            "epoch20: w = 1.922, loss = 0.06237914\n",
            "epoch21: w = 1.934, loss = 0.04506890\n",
            "epoch22: w = 1.944, loss = 0.03256231\n",
            "epoch23: w = 1.952, loss = 0.02352631\n",
            "epoch24: w = 1.960, loss = 0.01699772\n",
            "epoch25: w = 1.966, loss = 0.01228084\n",
            "epoch26: w = 1.971, loss = 0.00887291\n",
            "epoch27: w = 1.975, loss = 0.00641066\n",
            "epoch28: w = 1.979, loss = 0.00463169\n",
            "epoch29: w = 1.982, loss = 0.00334642\n",
            "epoch30: w = 1.985, loss = 0.00241778\n",
            "epoch31: w = 1.987, loss = 0.00174685\n",
            "epoch32: w = 1.989, loss = 0.00126211\n",
            "epoch33: w = 1.991, loss = 0.00091188\n",
            "epoch34: w = 1.992, loss = 0.00065882\n",
            "epoch35: w = 1.993, loss = 0.00047601\n",
            "epoch36: w = 1.994, loss = 0.00034392\n",
            "epoch37: w = 1.995, loss = 0.00024848\n",
            "epoch38: w = 1.996, loss = 0.00017952\n",
            "epoch39: w = 1.996, loss = 0.00012971\n",
            "epoch40: w = 1.997, loss = 0.00009371\n",
            "epoch41: w = 1.997, loss = 0.00006770\n",
            "epoch42: w = 1.998, loss = 0.00004891\n",
            "epoch43: w = 1.998, loss = 0.00003534\n",
            "epoch44: w = 1.998, loss = 0.00002553\n",
            "epoch45: w = 1.999, loss = 0.00001845\n",
            "epoch46: w = 1.999, loss = 0.00001333\n",
            "epoch47: w = 1.999, loss = 0.00000963\n",
            "epoch48: w = 1.999, loss = 0.00000696\n",
            "epoch49: w = 1.999, loss = 0.00000503\n",
            "epoch50: w = 1.999, loss = 0.00000363\n",
            "epoch51: w = 1.999, loss = 0.00000262\n",
            "epoch52: w = 2.000, loss = 0.00000190\n",
            "epoch53: w = 2.000, loss = 0.00000137\n",
            "epoch54: w = 2.000, loss = 0.00000099\n",
            "epoch55: w = 2.000, loss = 0.00000071\n",
            "epoch56: w = 2.000, loss = 0.00000052\n",
            "epoch57: w = 2.000, loss = 0.00000037\n",
            "epoch58: w = 2.000, loss = 0.00000027\n",
            "epoch59: w = 2.000, loss = 0.00000019\n",
            "epoch60: w = 2.000, loss = 0.00000014\n",
            "epoch61: w = 2.000, loss = 0.00000010\n",
            "epoch62: w = 2.000, loss = 0.00000007\n",
            "epoch63: w = 2.000, loss = 0.00000005\n",
            "epoch64: w = 2.000, loss = 0.00000004\n",
            "epoch65: w = 2.000, loss = 0.00000003\n",
            "epoch66: w = 2.000, loss = 0.00000002\n",
            "epoch67: w = 2.000, loss = 0.00000001\n",
            "epoch68: w = 2.000, loss = 0.00000001\n",
            "epoch69: w = 2.000, loss = 0.00000001\n",
            "epoch70: w = 2.000, loss = 0.00000001\n",
            "epoch71: w = 2.000, loss = 0.00000000\n",
            "epoch72: w = 2.000, loss = 0.00000000\n",
            "epoch73: w = 2.000, loss = 0.00000000\n",
            "epoch74: w = 2.000, loss = 0.00000000\n",
            "epoch75: w = 2.000, loss = 0.00000000\n",
            "epoch76: w = 2.000, loss = 0.00000000\n",
            "epoch77: w = 2.000, loss = 0.00000000\n",
            "epoch78: w = 2.000, loss = 0.00000000\n",
            "epoch79: w = 2.000, loss = 0.00000000\n",
            "epoch80: w = 2.000, loss = 0.00000000\n",
            "epoch81: w = 2.000, loss = 0.00000000\n",
            "epoch82: w = 2.000, loss = 0.00000000\n",
            "epoch83: w = 2.000, loss = 0.00000000\n",
            "epoch84: w = 2.000, loss = 0.00000000\n",
            "epoch85: w = 2.000, loss = 0.00000000\n",
            "epoch86: w = 2.000, loss = 0.00000000\n",
            "epoch87: w = 2.000, loss = 0.00000000\n",
            "epoch88: w = 2.000, loss = 0.00000000\n",
            "epoch89: w = 2.000, loss = 0.00000000\n",
            "epoch90: w = 2.000, loss = 0.00000000\n",
            "epoch91: w = 2.000, loss = 0.00000000\n",
            "epoch92: w = 2.000, loss = 0.00000000\n",
            "epoch93: w = 2.000, loss = 0.00000000\n",
            "epoch94: w = 2.000, loss = 0.00000000\n",
            "epoch95: w = 2.000, loss = 0.00000000\n",
            "epoch96: w = 2.000, loss = 0.00000000\n",
            "epoch97: w = 2.000, loss = 0.00000000\n",
            "epoch98: w = 2.000, loss = 0.00000000\n",
            "epoch99: w = 2.000, loss = 0.00000000\n",
            "epoch100: w = 2.000, loss = 0.00000000\n",
            "Prediction after training: f(5) = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " "
      ],
      "metadata": {
        "id": "73tiSZzi-JLS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}